{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prazd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prazd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prazd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prazd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join('data', 'alice_in_wonderland.txt')\n",
    "with open(file=text_path, mode='r', encoding='utf-8') as file:\n",
    "    book = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert book -> text corpus\n",
    "corpus = [line.strip() for line in book]\n",
    "corpus = ' '.join(corpus).split('THE END')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract CHAPTER's lines\n",
    "TRUE_CHAPTERS = [elem.strip().split(maxsplit=1) for elem in corpus.split('CHAPTER')[1:13]]\n",
    "\n",
    "# Get CHAPTER's text\n",
    "corpus = corpus.split('CHAPTER')[13:]\n",
    "corpus = [elem.replace(TRUE_CHAPTERS[i][0], '').replace(TRUE_CHAPTERS[i][1], '') for i, elem in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \" in sentences to get mush more clear ones\n",
    "corpus = [elem.replace('“', '').replace('”', '') for elem in corpus]\n",
    "\n",
    "# split chapters into sentences via '.', '?', '!'\n",
    "sentences_re = r'(?<=[.!?;])\\s+'\n",
    "corpus = [re.split(sentences_re, elem) for elem in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, and what is the use of a book, thought Alice without pictures or conversations?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "def get_words(text: str) -> str:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return word_tokenize(text)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "TAG_WORDNET_MAPPING = {\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"R\": wordnet.ADV\n",
    "}\n",
    "def get_wordnet_pos(words: List[str]) -> List[Tuple[str]]:\n",
    "    tags = nltk.pos_tag(words)\n",
    "    return [(elem[0], TAG_WORDNET_MAPPING.get(elem[1][0].upper(), wordnet.NOUN)) for elem in tags]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(word: str, pos: str) -> str:\n",
    "    return lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english') \n",
    "def stopwords_cleaner(words: List[str]) -> List[str]:\n",
    "    return [elem for elem in words if elem not in stopwords_english]\n",
    "\n",
    "def text_preprocessing(text: str) -> List[str]:\n",
    "    text = get_words(text=text)\n",
    "    text = lowercase(text=text)\n",
    "    text = tokenize(text=text)\n",
    "    text = get_wordnet_pos(words=text)\n",
    "    text = [lemmatize(word=elem[0], pos=elem[1]) for elem in text]\n",
    "    text = stopwords_cleaner(words=text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most important words from each chapter in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for temp_chapter in corpus:\n",
    "    temp_preprocessed_test_corpus = [text_preprocessing(text=elem) for elem in temp_chapter]\n",
    "    temp_preprocessed_test_corpus = [' '.join(elem) for elem in temp_preprocessed_test_corpus if len(elem) > 5]\n",
    "\n",
    "    temp_tfidf = TfidfVectorizer(\n",
    "        input=\"content\",\n",
    "        encoding=\"utf-8\",\n",
    "        lowercase=False,\n",
    "        preprocessor=None,\n",
    "        tokenizer=None,\n",
    "        analyzer=\"word\",\n",
    "        stop_words=None,\n",
    "        norm=\"l2\",\n",
    "        use_idf=True,\n",
    "        smooth_idf=True\n",
    "    )\n",
    "    temp_matrix = temp_tfidf.fit_transform(temp_preprocessed_test_corpus)\n",
    "    temp_words = temp_tfidf.get_feature_names_out()\n",
    "    temp_top_words = [temp_words[i] for i in np.argsort(temp_matrix.toarray().sum(axis=0))[-10:] if temp_words[i] != 'alice']\n",
    "    top_words.append(temp_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down the Rabbit-Hole: ['try', 'find', 'way', 'get', 'see', 'think', 'say', 'little', 'go']\n",
      "The Pool of Tears: ['one', 'know', 'come', 'foot', 'im', 'mouse', 'little', 'go', 'say']\n",
      "A Caucus-Race and a Long Tale: ['seem', 'think', 'look', 'one', 'get', 'know', 'dodo', 'mouse', 'say']\n",
      "The Rabbit Sends in a Little Bill: ['rabbit', 'grow', 'one', 'come', 'say', 'little', 'make', 'get', 'go']\n",
      "Advice from a Caterpillar: ['pigeon', 'minute', 'get', 'well', 'think', 'im', 'caterpillar', 'little', 'say']\n",
      "Pig and Pepper: ['see', 'footman', 'like', 'get', 'think', 'little', 'cat', 'go', 'say']\n",
      "A Mad Tea-Party: ['go', 'well', 'take', 'march', 'hare', 'time', 'hatter', 'dormouse', 'say']\n",
      "The Queen’s Croquet-Ground: ['three', 'see', 'think', 'come', 'go', 'king', 'look', 'queen', 'say']\n",
      "The Mock Turtle’s Story: ['make', 'think', 'duchess', 'queen', 'gryphon', 'go', 'mock', 'turtle', 'say']\n",
      "The Lobster Quadrille: ['could', 'dance', 'go', 'lobster', 'would', 'gryphon', 'turtle', 'mock', 'say']\n",
      "Who Stole the Tarts?: ['make', 'dormouse', 'begin', 'one', 'king', 'court', 'look', 'hatter', 'say']\n",
      "Alice’s Evidence: ['look', 'go', 'write', 'give', 'know', 'would', 'jury', 'king', 'say']\n"
     ]
    }
   ],
   "source": [
    "for temp_chapter, elem in zip(TRUE_CHAPTERS, top_words):\n",
    "    print(f'{temp_chapter[1]}: {elem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Down the Rabbit-Hole: Find the way\n",
    "   \n",
    "2. The Pool of Tears: One little mouse\n",
    "   \n",
    "3. A Caucus-Race and a Long Tale: Looking for dodo\n",
    "   \n",
    "4. The Rabbit Sends in a Little Bill: Grown rabbit\n",
    "   \n",
    "5. Advice from a Caterpillar: Сaterpillar thoughts\n",
    "   \n",
    "6. Pig and Pepper: Little cat\n",
    "   \n",
    "7. A Mad Tea-Party: Dormouse\n",
    "   \n",
    "8. The Queen’s Croquet-Ground: King and Queen\n",
    "   \n",
    "9.  The Mock Turtle’s Story: Turtle's mock\n",
    "    \n",
    "10. The Lobster Quadrille: Dance\n",
    "    \n",
    "11. Who Stole the Tarts?: Court\n",
    "    \n",
    "12. Alice’s Evidence: Looking for evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most used verbs in sentences with Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change some preprocessing functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(word: str, pos: str) -> str:\n",
    "    return (lemmatizer.lemmatize(word, pos), pos)\n",
    "\n",
    "stopwords_english = stopwords.words('english') \n",
    "def stopwords_cleaner(words: List[str]) -> List[str]:\n",
    "    return [(elem[0], elem[1]) for elem in words if elem[0] not in stopwords_english]\n",
    "\n",
    "def text_preprocessing(text: str) -> List[str]:\n",
    "    text = get_words(text=text)\n",
    "    text = lowercase(text=text)\n",
    "    text = tokenize(text=text)\n",
    "    text = get_wordnet_pos(words=text)\n",
    "    text = [lemmatize(word=elem[0], pos=elem[1]) for elem in text]\n",
    "    text = stopwords_cleaner(words=text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most used verbs in sentences with Alice: [('say', 192), ('think', 61), ('go', 57), ('look', 43), ('get', 42), ('begin', 33), ('see', 28), ('know', 20), ('find', 19), ('make', 18)]\n"
     ]
    }
   ],
   "source": [
    "counter = {}\n",
    "for temp_chapter in corpus:\n",
    "    for temp_sentence in temp_chapter:\n",
    "        temp_preprocessed_sentence = text_preprocessing(text=temp_sentence)\n",
    "        temp_words = [elem[0] for elem in temp_preprocessed_sentence]\n",
    "        if 'alice' not in temp_words:\n",
    "            continue\n",
    "        for word in temp_preprocessed_sentence:\n",
    "            if word[1] == 'v' and word[0] != 'alice':\n",
    "                counter[word[0]] = counter.get(word[0], 0) + 1\n",
    "\n",
    "print(f\"Top 10 most used verbs in sentences with Alice: {sorted(list(counter.items()), key=lambda elem: -elem[1])[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
